{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, StructType,StructField\n",
    "\n",
    "master = \"local[*]\"\n",
    "app_name = \"hipages\"\n",
    "\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "\n",
    "# Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and produce first csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- ip: string (nullable = true)\n",
      " |    |-- session_id: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df = spark.read.json(\"source_event_data.json\")\n",
    "# df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "# No null values as per above\n",
    "\n",
    "# Specifying the schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"user\", StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"ip\", StringType(), True),\n",
    "        StructField(\"session_id\", StringType(), True),\n",
    "    ]), True),\n",
    "    StructField(\"action\",  StringType(), True),\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "# Read the file\n",
    "# if there are many files we can read thm all at once by specifying the file name as *.json - assume all the files related to event data\n",
    "df =spark.read.format(\"json\").option(\"header\", \"True\").schema(schema).load(\"./source_event_data.json\")\n",
    "\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------+--------------------+------------+--------------+\n",
      "|user_id|          timestamp|    url_level1|          url_level2|  url_level3|        action|\n",
      "+-------+-------------------+--------------+--------------------+------------+--------------+\n",
      "|  56456|02/02/2017 20:22:00|hipages.com.au|            articles|        NULL|     page_view|\n",
      "|  56456|02/02/2017 20:23:00|hipages.com.au|             connect| sfelectrics|     page_view|\n",
      "|  56456|02/02/2017 20:26:00|hipages.com.au|get_quotes_simple...|        NULL|     page_view|\n",
      "|  56456|01/03/2017 20:21:00|hipages.com.au|           advertise|        NULL|  button_click|\n",
      "|  56456|02/02/2017 20:12:34|hipages.com.au|              photos|   bathrooms|     page_view|\n",
      "|  56456|01/01/2017 20:22:00|hipages.com.au|                find|electricians|list_directory|\n",
      "|  56456|02/02/2017 20:26:07|hipages.com.au|            articles|        NULL|     page_view|\n",
      "|  56456|02/02/2017 20:22:00|hipages.com.au|             connect| sfelectrics|  button_click|\n",
      "|  56456|02/02/2017 20:23:00|hipages.com.au|get_quotes_simple...|        NULL|  button_click|\n",
      "|  56456|02/02/2017 20:26:00|hipages.com.au|                find|electricians|         claim|\n",
      "|  56456|01/03/2017 20:21:00|hipages.com.au|                find|    plumbers|     page_view|\n",
      "|  56456|02/02/2017 20:12:34|hipages.com.au|                find|    plumbers|list_directory|\n",
      "|  56456|01/01/2017 20:22:00|hipages.com.au|get_quotes_simple...|        NULL|     page_view|\n",
      "|  56456|02/02/2017 20:26:07|hipages.com.au|                find|electricians|  button_click|\n",
      "|  56456|02/02/2017 20:22:00|hipages.com.au|get_quotes_simple...|        NULL|  button_click|\n",
      "|  56456|02/02/2017 20:23:00|hipages.com.au|                find|electricians|         claim|\n",
      "|  56456|02/02/2017 20:26:00|hipages.com.au|            articles|        NULL|     page_view|\n",
      "|  56798|01/03/2017 20:21:00|hipages.com.au|            articles|        NULL|     page_view|\n",
      "|  56798|02/02/2017 20:12:34|hipages.com.au|             connect| sfelectrics|     page_view|\n",
      "|  56798|01/01/2017 20:22:00|hipages.com.au|get_quotes_simple...|        NULL|     page_view|\n",
      "+-------+-------------------+--------------+--------------------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the user id\n",
    "df=df.withColumn(\"user_id\",df['user'][\"id\"])\n",
    "\n",
    "# clean the url with regex, remove http://www.,www.,https://www. and w3. patterns \n",
    "# then split the string with / then assign first 3 url lelvels accordingly\n",
    "df = df.withColumn(\"cleaned_url\",F.regexp_replace(df.url,\"(http://www.|www.|https://www.|w3.)\", \"\"))\n",
    "df=df.withColumn(\"cleaned_url\", F.split(F.col(\"cleaned_url\"), \"/\"))\n",
    "df =df.withColumn(\"url_level1\", df['cleaned_url'][0])\n",
    "df =df.withColumn(\"url_level2\", df['cleaned_url'][1])\n",
    "df =df.withColumn(\"url_level3\", df['cleaned_url'][2])\n",
    "\n",
    "# select the required columes and replace null values as NULL\n",
    "df =df.select(F.col(\"user_id\"),F.col(\"timestamp\"),F.col(\"url_level1\"),F.col(\"url_level2\"),F.col(\"url_level3\"),F.col(\"action\")).na.fill(value='NULL')\n",
    "\n",
    "#write to a csv file\n",
    "df.write.format(\"csv\").save(\"./table_1\")\n",
    "df.show()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing the second csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------+--------------+----------+\n",
      "|time_bucket|          url_level1|          url_level2|        action|activity_count|user_count|\n",
      "+-----------+--------------------+--------------------+--------------+--------------+----------+\n",
      "| 2017020220|      hipages.com.au|              photos|     page_view|            30|        30|\n",
      "| 2017020220|      hipages.com.au|            articles|     page_view|            59|        30|\n",
      "| 2017020420|      hipages.com.au|get_quotes_simple...|     page_view|            28|        28|\n",
      "| 2017020220|      hipages.com.au|                find|  button_click|             1|         1|\n",
      "| 2017020220|      hipages.com.au|           advertise|  button_click|            29|        29|\n",
      "| 2017010120|      hipages.com.au|get_quotes_simple...|     page_view|            30|        30|\n",
      "| 2017050220|  randompages.com.au|                find|  button_click|             1|         1|\n",
      "| 2017030120|      hipages.com.au|           advertise|  button_click|             1|         1|\n",
      "| 2017020220|     petpages.com.au|                find|     page_view|             1|         1|\n",
      "| 2017020220|      hipages.com.au|             connect|     page_view|            30|        30|\n",
      "| 2017030120|      hipages.com.au|            articles|     page_view|            29|        29|\n",
      "| 2017020220|      hipages.com.au|                find|     page_view|            26|        26|\n",
      "| 2017050220|      hipages.com.au|                find|  button_click|            27|        27|\n",
      "| 2017020220|myhelpfulpages.co...|            articles|     page_view|             1|         1|\n",
      "| 2017020220|      hipages.com.au|get_quotes_simple...|     page_view|             1|         1|\n",
      "| 2017010120|      hipages.com.au|                find|         claim|            27|        27|\n",
      "| 2017020220|      hipages.com.au|             connect|  button_click|             1|         1|\n",
      "| 2017030120|      hipages.com.au|             connect|  button_click|            28|        28|\n",
      "| 2017020220|    somepages.com.au|                find|     page_view|             1|         1|\n",
      "| 2017010120|  usefulpages.com.au|                find|         claim|             1|         1|\n",
      "| 2017040220|      hipages.com.au|                find|list_directory|            28|        28|\n",
      "| 2017030120|      hipages.com.au|                find|     page_view|             1|         1|\n",
      "| 2017020220|      hipages.com.au|                find|         claim|             2|         1|\n",
      "| 2017020220|      hipages.com.au|                find|list_directory|            30|        30|\n",
      "| 2017120220|      hipages.com.au|                find|         claim|            28|        28|\n",
      "| 2017080220|      hipages.com.au|get_quotes_simple...|  button_click|            28|        28|\n",
      "| 2017010120|      hipages.com.au|                find|list_directory|             1|         1|\n",
      "| 2017020220|      hipages.com.au|get_quotes_simple...|  button_click|            30|        29|\n",
      "+-----------+--------------------+--------------------+--------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DO the aggredation\n",
    "# Once the first step is completed\n",
    "\n",
    "df=  df.withColumn('timestamp', F.to_timestamp('timestamp', format='dd/MM/yyyy HH:mm:ss')) \n",
    "df=df.withColumn(\"time_bucket\", F.date_format(F.col(\"timestamp\").cast(\"timestamp\"), \"yyyyMMddHH\"))\n",
    "df=df.groupBy(\"time_bucket\", \"url_level1\",\"url_level2\",\"action\").agg(F.count('action').alias('activity_count'),F.countDistinct('user_id').alias('user_count'))\n",
    "\n",
    "# save to a csv file\n",
    "df.write.format(\"csv\").save(\"./table_2\")\n",
    "\n",
    "df.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "    1. Spark is preferred/used for the data transformation/etl process, it can operate in a cluster environment and efficient on performing jobs in parallel\n",
    "    2. we can use scheduling tool like air flow to perform above operations in scheduled intervals\n",
    "    3. Parquet file types are profred and it's a column oriented format and column pruning is a big performance improvement so we can perform the operations we need for the selected columns than reading the entire rows, further it has information about the column so we no need to perform operations to get those info (ex: null count etc..\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
